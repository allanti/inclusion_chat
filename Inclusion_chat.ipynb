{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVnyQqc4MtW+Czo4F1+FMe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allanti/inclusion_chat/blob/main/Inclusion_chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RjM3gWoGxM1M"
      },
      "outputs": [],
      "source": [
        "pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import textwrap\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('SECRET_KEY')\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "generation_config = {\n",
        "    \"candidate_count\": 1,\n",
        "    \"temperature\":0.5\n",
        "}\n",
        "\n",
        "safety_settings = {\n",
        "    \"HARASSMENT\": \"BLOCK_NONE\",\n",
        "    \"HATE\": \"BLOCK_NONE\",\n",
        "    \"SEXUAL\": \"BLOCK_NONE\",\n",
        "    \"DANGEROUS\": \"BLOCK_NONE\",\n",
        "}\n",
        "\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.0-pro\",\n",
        "                              generation_config = generation_config,\n",
        "                              safety_settings = safety_settings)\n",
        "\n",
        "chat = model.start_chat()\n",
        "\n",
        "display(Markdown(\"**Olá, seja bem-vindo, é uma felicidade poder falar com você.**\"))\n",
        "display(Markdown(\"Primeiro faremos algumas perguntas para direcionar nossa conversa, não se preocupe será rápido\"))\n",
        "\n",
        "questions = [\n",
        "    \"De que pais você está falando ?\",\n",
        "    \"Voce precisa de ajuda para você ou para outra pessoa?\",\n",
        "    \"Gostaria de apoio a qual tipo de deficiencia?\",\n",
        "    \"Você está enfrentando algum desafio específico no momento? Descreva de forma detalhada\",\n",
        "]\n",
        "\n",
        "response = \"\"\n",
        "count = 0;\n",
        "responses = []\n",
        "\n",
        "# Loop de perguntas\n",
        "while response.lower() != \"ok\":\n",
        "    if count > 3:\n",
        "        display(Markdown(\"**Para continuarmos digite: ok**\"))\n",
        "        response = input(\"Resposta: \")\n",
        "    else:\n",
        "        display(Markdown(f\"**{questions[count]}**\"))\n",
        "        response = input(\"Resposta: \")\n",
        "        print(\"\\n\")\n",
        "        responses.append(response)\n",
        "        count += 1\n",
        "\n",
        "\n",
        "# Parametrização do modelo\n",
        "guide_prompt = f\"Considere essas informações antes de responder: Minha localidade é {responses[0]}. Responda no idioma dessa localidade, Preciso de ajuda para {responses[1]}. Deficiencia{responses[2]} Pergunta:Estou com dificudade em {responses[3]}, me auxilie no que fazer e depois me de contatos para serviços de apoio em minha localidade\"\n",
        "\n",
        "display(Markdown(f\"**Espero que essas informações te ajudem. Se precisar de qualquer outra informação ou ajuda estarei aqui para auxiliar. Para finalizar a interação digite fim**\\n\"))\n",
        "\n",
        "# Geração das informações\n",
        "guide_response = chat.send_message(guide_prompt)\n",
        "display(Markdown(guide_response.text.replace(\"*\", \"\")))\n",
        "\n",
        "# Interação contínua com o modelo\n",
        "while True:\n",
        "    prompt = input(\"Pergunta: \")\n",
        "    if prompt.lower() == \"fim\":\n",
        "        break\n",
        "    response = chat.send_message(prompt)\n",
        "    display(Markdown(f\"**Resposta:** {response.text.replace('*', '')}\\n\"))\n"
      ],
      "metadata": {
        "id": "Y4IlPbumxU1f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}